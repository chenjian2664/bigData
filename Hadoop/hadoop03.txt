

Hadoop脚本分析
---------------------
	1.start-all.sh
	hadoop的按住目录下:
	libexec/hadoop-config.sh //设置变量
	sbin/start-dfs.sh --config $HADOOP_CONF_DIR //启动hdfs
	sbin/start-yarn.sh --config $HADOOP_CONF_DIR //启动yarn

	2.libexec/hadoop-config.sh			//设置变量
		COMMON_DIR
		...
		HADOOP_CONFIG_DIR=...
		HEAD_SIZE=1000m
		CLASSPATH=...

	3.sbin/start-dfs.sh --config $HADOOP_CONF_DIR
		1.libexec/hdfs-config.sh
			#获得名称节点主机名
		2.NAMENODES=hdfs getconf -namenodes
		3.启动名称节点
		"$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
			--config "$HADOOP_CONF_DIR" \
			--hostnames "NAMENODES" \
			--script "$bin/hdfs" start namenode $nameStartOpt
		4.启动数据节点
		"$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
			--config "$HADOOP_CONF_DIR" \
			--script "$bin/hdfs" start datanode $dataStartOpt
		5.启动2nn
		"$HADOOP_PREFIX/sbin/hadoop-daemons.sh" \
			--config "$HADOOP_CONF_DIR" \
			--hostnames "$SECONDARY_NAMENODES" \
			--script "$bin/hdfs" start secondarynamenode

	4.libexec/hdfs-config.sh
		内部调用hadoop-config.sh
		也就是第一步都是设置变量

	5.sbin/hadoop-daemons.sh	//启动守护进程
		1.libexec/hdfs-config.sh  //执行配置脚本
		2.exec "$bin/salves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_PREFIX" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"
		3.循环利用ssh登录远程主机执行相应命令

		关于 $bin/hadoop-daemon.sh脚本
			会调用hadoop-config.sh 配置变量
			会执行[bin/hdfs]命令
			bin/hdfs 最终利用java命令 
	

	6.bin/hadoop 
		hadoop-config.sh
		java ...
	7.bin/hdfs
		hadoop-config.sh
		java ...
	
常用命令
----------------
	1.格式化系统
		hadoop namenode -format
	2.hadoop fs == hdfs dfs
	3.hdfs dfs -put == hdfs dfs -copyFromLocal
	4.hdfs dfs -get == hdfs dfs -copyToLocal
	5.剪切 hdfs dfs -moveToLocal // hdfs dfs -moveFromLocal

hadoop配置信息
-----------------
[hdfs-site.xml]
dfs.namenode.name.dir
namenode的本地目录配置成多个，则每个目录存放内容相同，可靠

dfs.datanode.data.dir
决定本地文件系统中数据中块数据存在哪里，如果存储类型没有设置会默认存在磁盘上面。
datanode的本地目录配置成多个，不是副本是存储的位置。


hadoop hdfs块大小定义为128M
----------------------------
磁盘寻道时间=10ms 
磁盘IO速率：100Ms


!!!hadoop中报有关native的错误 一般都与hadoop的版本有关，所有安装hadoop最好保证版本一致，安装完整。
windows编程连接linux服务器有出先winutils.exe缺失的异常，是hadoop/bin目录下没有winutis.exe和hadoop.dll所致，下载版本一致的windowhadoop 配置HADOOP_HOME和path=$HADOOP_HOME下的bin目录


[vim 和 shell的一些快捷键]
vim : e shift+e 下一个单词跳跃 shift大跳 b shift+b 会跳上一个单词
5gg 8gg 分别跳到5和8行
^跳至行首$跳到行尾
w跳到下一个字首
shell:ctrl+a 命令首 ctrl+e命令尾 alt+箭头一个单词一个单词跳跃
ctrl+r 搜索使用过的最近的命令

查询集群


package com.chenjian.hdfs.test;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.io.IOUtils;
import org.junit.Before;
import org.junit.Test;

import java.io.*;
import java.lang.reflect.Method;


/**
 * 使用hadoop实现文件操作
 */
public class TestFileSystem {
    private FileSystem fileSystem;
    @Before
    public void initConf() {
       Configuration configuration = new Configuration();
        try {
            fileSystem = FileSystem.get(configuration);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
    @Test
    public void writeFile() throws Exception {
        Path path = new Path("hdfs://s100:8020/user/test.txt");
        //数据输出流
        FSDataOutputStream fsDataOutputStream = fileSystem.create(path);
        fsDataOutputStream.write("helloworld".getBytes());
        fsDataOutputStream.close();
        System.out.println("over");
    }

    @Test
    public void readFile() throws IOException {
        Path path = new Path("hdfs://s100:8020/user/chenjian/data/hello.txt");
        //数据输入流
        FSDataInputStream fsDataInputStream = fileSystem.open(path);
        byte[] buf = new byte[1024];
        int len = fsDataInputStream.read(buf);
        FileOutputStream fileOutputStream = new FileOutputStream("C:\\Users\\Administrator\\Desktop\\read.txt");

        fileOutputStream.write(buf, 0, len);
        System.out.println("over");
    }

    /**
     * 利用hadoop API 进行读取数据
     */
    @Test
    public void readFileInHadoopAPI() throws IOException {
        Path path = new Path("hdfs://s100:8020/user/chenjian/data/hello.txt");
        //数据输入流
        FSDataInputStream fsDataInputStream = fileSystem.open(path);
        FileOutputStream fileOutputStream = new FileOutputStream("C:\\Users\\Administrator\\Desktop\\read.txt");
        IOUtils.copyBytes(fsDataInputStream, fileOutputStream, 1024);

        System.out.println("over");
    }
    /**
     * 利用Hadoop API写数据
     */
    @Test
    public void writeFileInHadoopAPI() throws Exception {
        InputStream inputStream = new FileInputStream("C:\\Users\\Administrator\\Desktop\\read.txt");
        FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path("hdfs://s100:8020/user/read.txt"),(short)3);
        IOUtils.copyBytes(inputStream,fsDataOutputStream,1024);
        System.out.println("over");
    }

    /**
     * mkdirs method 使用
     */
    @Test
    public void mkdirTest() throws IOException {

        boolean b = fileSystem.mkdirs(new Path("/user/directoryTest"), FsPermission.getDefault());
        System.out.println(b);
    }
    /**
     * 遍历根目录
     * 调用FileStatus所有无参数的函数
     */
    @Test
    public void fileStatus() throws Exception {
        FileStatus fileStatus = fileSystem.getFileStatus(new Path("/"));
        Class cls = FileStatus.class;
        Method[] methods = cls.getDeclaredMethods();

        for (Method method : methods) {
            String mname = method.getName();
            Class<?>[] cs = method.getParameterTypes();
            //无参函数
            if (mname.startsWith("get") && (cs == null || cs.length == 0)) {
                //getSymlink函数无法调用
                if (!mname.equals("getSymlink")) {
                    System.out.println(mname + "() = " + method.invoke(fileStatus, null));
                }
            }
        }
    }
}
