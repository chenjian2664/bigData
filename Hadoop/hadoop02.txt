
1.namenode
	http://localhost:50070/
2.datanode
	http://localhost:50075/
3.2nn
	http://localhost:50090/

[SSH免密码登录]
1.本机执行
	ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	可以让本机免密码登录本机
2.ssh-copy-id ~/.ssh/id_rsa.pub chenjian@s102

scp
------------
	拷贝远程文件复制,基于ssh

scp难以拷贝链接link

rsync
---------
	支持link复制 
	rsync -vrl 源文件 chenjian@s104:/路径
	rsync -vrl /usr/local/java  chenjian@s104:/usr/local/java 


自定义脚本xsync
---------------
	循环复制文件到所有节点的相同目录下
	rsync -rvl hello.txt
	xsync hello.txt
	[/usr/local/bin/xsync]
	#!/bin/bash
	pcount=$#
	if (( pcount < 1 )); then
		echo no args;
		exit;
	fi

	p1=$1;
	fname=`basename $p1`;
	
	#获得上级目录的绝对路径
	pdir=`cd -P $(dirname $p1); pwd`
	
	cuser=`whoami`
	#循环
	for (( host=101; host<105; host=host+1 )) ; do
		rsync -rvl $pdir/$fname $cuser@$s$host:$pdir
	done

自定义脚本xcall 
--------------------------
	在所有主机上执行相同命令 本机s100服务器s101-s104
	[/usr/local/bin/xsync]
	#!/bin/bash
	pcount=$#
	if (( pcount < 1 )); then
		echo no args;
		exit;
	fi
  
	for (( host=101; host<105; host=host+1 ));	do
		echo ---------------s$host-------------
		ssh s$host $@
	done


整理hadoop的所有类库
1.解压缩hadoop-2.7.5.tar.gz到目录下
2.整理jar包 将source.jar test.jar 以及所有jar包整理出来
3.抽取所有配置文件
	[core-default.xml]
	hadoop-common-2.7.5.jar/core-default.xml
	[hdfs-default.xml]
	hadoop-hdfs-2.7.5.jar/hdfs-default.xml
	[yarn-default.xml]
	hadoop-yarn-2.7.5.jar/yarn-default.xml
	[mapred-default.xml]
	hadoop-mapreduce-client-core-2.7.5.jar/mapred-default.xml
	


